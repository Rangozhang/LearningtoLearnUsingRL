我现在有一个疑问。我的目的是找到optimizee一个初始learning rate，使得如下两个条件成立：
1. 我有两个action={保持lr不变，让lr减小}。在前期，我希望让第一个action的reward比较大，在后期，我希望让第二个action的reward比较大。(因为常识是lr下降对训练有好处)
2. 于此同时，对于这个learning rate而言，如果我在第20个epoch开始进行lr下降，loss的结果应该比不安排lr下降要低。

解答：
lr初始值越大，lr下降的时间点越靠前。
在初期，小的lr会让loss下降的慢（只要不是过大的lr，比如说在这个实验中lr=1。

过大的lr：0.8, 1  (> 0.5 初期) 0.3, 0.4 (< 0.5 初期)

结论：用0.5开始的时候，初始情况下的action0的reward会比较高，后期action1的reward会比较高。

Question: Is it due to general gradient descent?
Looks like the reward is much more constant when applying adam gradient descent.

When using adam gradient descent, according to figure, 4e-3 is the best initial learning rate to choose.

Still not a clue why two actions are not getting reasonable results:
1) check the distribution of reward
2) action_freq: accumulate a few steps instead of just one step for an action to get the reward, which means perform an action each a few steps

Found out that the q net is actually learning, all that needs to do is to find a good reward so that the decrease of the loss can be represented by it.

1: inverseloss 1/2  loss decrease at the beginning
2: logdiff     1/2  not working
3: inverseloss 0.97 lr decrease pretty decently
4: logdiff     0.97 not working

1: 0.9stay 1/x
2: 0.9stay -log
3: 0.99res 1/x
4: 0.99res -log
5: 0.99res 1/x 550 300ep

1: 0.99res 1/x
2: 0.99res 1/x   1-0.001
3: 0.99res 1/x*2 1-0.001

1487732245 1: 0.99res 1/x mem500		: stick to decrease action
1487732343 2: 0.99res 1/x mem500 lr3e-3 : still keep decreasing
1487732711 3: 0.99sty 1/x mem500		: stick to decrease, which yields best performance no doubt.
1487736385 3: 0.99res 1/x mem500 lr1e-3 : stick to decrease, which yields worse performance... :((
1487745551 3: 0.99sty 1/x mem2500 lr1e-3 : stick to decrease...

1487748341 3: 0.97sty 1/x mem2500 lr1e-3: a better param settings
1487748634 2: 0.97sty 1/x mem2500 lr1e-3 e jump

Conclusion: training is not enough.

1487818092 1: 0.97sty 1/x lr4e-3 2000ep           : Not as good as r4lastStep, stoped

1487842613 1: 0.97sty 1/x lr4e-3 500ep  r4lastStep newest version: 
1487818034 2: 0.97sty 1/x lr4e-3 2000ep r4lastStep:                     e is not jumping from 0.6 to 0
1487831898 3: 0.97sty 1/x'lr4e-3 2000ep r4lastStep:                     reward is calculated by whole train image set; e is not jumping from 0.6 to 0
